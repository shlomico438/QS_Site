from gevent import monkey
monkey.patch_all()
from flask import Flask, render_template, request, jsonify
from flask_socketio import SocketIO, join_room
import json
import requests  # Added for RunPod API calls
import time
import logging
import os
# --- CONFIGURATION ---
SIMULATION_MODE = False  # <--- Set to False when deploying to Koyeb
S3_BUCKET = os.environ.get("S3_BUCKET")
# Note: We don't need the keys here, we need them inside the function

app = Flask(__name__)
app.config['SECRET_KEY'] = 'secret_scribe_key_123'
app.config['MAX_CONTENT_LENGTH'] = 500 * 1024 * 1024

# Configuration for automation
RUNPOD_API_KEY = os.environ.get('RUNPOD_API_KEY')
RUNPOD_ENDPOINT_ID = os.environ.get('RUNPOD_ENDPOINT_ID')
BUCKET_NAME = "quickscribe-v2-12345"

# Strict settings to keep connections alive
socketio = SocketIO(app,
    cors_allowed_origins="*",
    async_mode='gevent',
    transports=['websocket'],
    ping_timeout=600,
    ping_interval=20,
    manage_session=False
)
print("--- STEP 11: socket io ---")

# --- GLOBAL CACHE ---
job_results_cache = {}

logging.basicConfig(level=logging.INFO)

# --- MOCK ROUTE FOR LOCAL DEBUGGING ---
@app.route('/api/mock-upload', methods=['PUT'])
def mock_upload():
    print("È¶ÉÊï≠ SIMULATION: Fake file upload received!")
    return "", 200

@app.after_request
def add_security_headers(resp):
    resp.headers['Cross-Origin-Embedder-Policy'] = 'require-corp'
    resp.headers['Cross-Origin-Opener-Policy'] = 'same-origin'
    return resp

@app.errorhandler(413)
def request_entity_too_large(error):
    return jsonify({"status": "error", "message": "File too large. Max 500MB."}), 413

@app.errorhandler(Exception)
def handle_exception(e):
    if hasattr(e, 'code'):
        return jsonify({"status": "error", "message": str(e.description)}), e.code
    logging.error(f"Unexpected Server Error: {str(e)}")
    return jsonify({"status": "error", "message": "Internal server error."}), 500

# --- WEB ROUTES ---
@app.route('/')
def index(): return render_template('index.html')

@app.route('/about')
def about(): return render_template('about.html')

@app.route('/blog')
def blog(): return render_template('blog.html')

@app.route('/contact')
def contact():
    return render_template('contact.html')

# --- UPLOAD & TRIGGER API ---
import time  # Ensure time is imported at the top of your file



def trigger_gpu_job(job_id, s3_key, num_speakers, language, task):
    """Initiates the RunPod Serverless task with 5 parameters and retry logic."""
    if not RUNPOD_API_KEY or not RUNPOD_ENDPOINT_ID:
        error_text = "RunPod keys not found in environment variables."
        print(f"ERROR: {error_text}")
        raise Exception(error_text)

    url = f"https://api.runpod.ai/v2/{RUNPOD_ENDPOINT_ID}/run"
    headers = {
        "Authorization": f"Bearer {RUNPOD_API_KEY}",
        "Content-Type": "application/json"
    }

    # --- SMART PROMPT LOGIC ---
    # "Primes" the AI to stick to the correct output language
    if task == "translate":
        # Force it to think in English
        prompt = "Here is the complete English translation of the Hebrew interview."
    else:
        # Force it to stay in Hebrew (helps with technical terms)
        prompt = "ËÆõË∞ûËÆõ ËΩ¨ËØ™ËØáËÆúËØá ËØ™ËØáËÆó Á†ñËØá ËÆõÁ†ñËÆ¨ËÆûËÆõ ËÆòÊ≥®ËÆò‰∏ìËÆ¨ËΩ¨, ËÆªËÆúËØáËØá È©ªËÆ¨‰ΩèËÆúÊãΩ ËØ™ËÆöËÆúËÆ¨ÊãΩ."

    # 3. Build RunPod Payload
    payload = {
        "input": {
            "jobId": data.get('jobId'),  # Match 'job_id = job_input.get("jobId")'
            "s3Key": data.get('s3Key'),  # Match 's3_key = job_input.get("s3Key")'
            "task": data.get('task', 'transcribe'),
            "language": data.get('language', 'he'),
            "num_speakers": int(data.get('speakerCount', 2)),
            "diarization": data.get('diarization', False)
        }
    }

    max_retries = 3
    last_error = ""

    for attempt in range(1, max_retries + 1):
        try:
            print(f"DEBUG: Triggering GPU Attempt {attempt}/{max_retries} for {job_id}...")
            response = requests.post(url, json=payload, headers=headers, timeout=10)

            if response.status_code in [200, 201]:
                print(f"GPU TRIGGERED SUCCESSFULLY: {response.json()}")
                return
            else:
                last_error = f"Status {response.status_code}: {response.text}"
                print(f"DEBUG: Attempt {attempt} failed - {last_error}")

        except Exception as e:
            last_error = str(e)
            print(f"DEBUG: Attempt {attempt} Exception - {last_error}")

        if attempt < max_retries:
            time.sleep(1)

    raise Exception(f"Failed to trigger GPU after {max_retries} attempts. Last error: {last_error}")


# --- Add this to app.py ---

@app.route('/api/check_status/<job_id>', methods=['GET'])
def check_job_status(job_id):
    if SIMULATION_MODE:
        # Return a fake completed response immediately
        return jsonify({
            "status": "completed",
            "result": {
                "segments": [
                    {"start": 0.0, "end": 2.5, "text": "Á†ñËØáËÆúËØê ËÆûËÆòËÆ¨ËÆòËΩ¨ËÆ¨. ËÆóËÆù ËÆùËÆóËΩ¨ ËÆõ‰ΩèÈ©ª‰∏ìËÆ¨ËÆ¨ËÆõ Á†ñËØáËÆ±? ËÆªË∞â. ËØ™ËÆõ Á†ñËØáËÆúËØ™ËÆ±? ", "speaker": "SPEAKER_00"},
                    {"start": 3.0, "end": 6.0, "text": "ËØ™ËΩ¨ËÆú‰ΩèËÆªËØáËΩ¨, ËÆóË∞ûËÆ¨ ËØáËÆó ËØ™ËÆúÁà™ËÆóËΩ¨ ËÆóËΩ¨ ËÆõ‰ΩèÈ©ª‰∏ì Á†ñË∞ûËΩ¨ËΩ¨ ËØáËÆ¨ ËÆõËØ™Ë∞ûÁà™ËÆû, ËÆóËÆ¨ËÆ± ËÆùËÆõ Ë∞ûÊãΩ‰∏ìËÆó? ‰ΩèËÆúËÆö ËÆõ‰ΩèËÆ¨È©ªËÆú‰∏ì ËÆõËØ™Ë∞ûÁà™ËÆû? ", "speaker": "SPEAKER_01"},
                    {"start": 7.0, "end": 10.0, "text": "ËÆªË∞â, ËØáËÆó ËÆûÁ†ñËÆúËÆò, ËÆóË∞ûËÆ¨ ËÆóËØ™Áà™ËÆó ËÆóËÆúËΩ¨ËÆú, ËÆóË∞ûËÆ¨ ËÆóËØ™Áà™ËÆó ËÆóËÆúËΩ¨ËÆú. ËØáËÆó, ËÆóË∞ûËÆ¨ ËÆóËÆòËÆ¨ËÆó ËØáËÆ±, ËÆ¨Á†ñ ËØáËÆ¨ ËØ™ËØáËÆó. ËÆóËÆòËØá ËÆªËÆò‰∏ì Ë∞ûËΩ¨ËΩ¨ËÆ¨, ËÆóËÆòËØá ËÆªËÆò‰∏ì Ë∞ûËΩ¨ËΩ¨ËÆ¨ ", "speaker": "SPEAKER_00"},
                    {"start": 10.5, "end": 15.0, "text": "ËØáËÆªËØ™ËÆõ ËÆóËΩ¨ ËÆõ‰ΩèÈ©ª‰∏ì Á†ñËØáËÆ± ËÆ¨ËÆúÁà™ËÆóËÆúËΩ¨ ËØ™ËÆôËÆö‰∏ìË∞â, ËÆúÁ†ñËØ™ËÆûËÆõ ‰∏ìËÆòËÆõ. ËÆóËÆ¨ËÆùËÆõ ËÆªËÆ¨Á•ù, ËÆóË∞ûËÆ¨ ËÆóËÆòËÆ¨ËÆó ËØáËÆ±, ËÆ¨Á†ñ ËØáËÆ¨ ËÆõËØ™ËÆúË∞â ËÆò‰ΩèÈ©ª‰∏ìËÆ¨ËÆ¨ËÆõ", "speaker": "SPEAKER_01"},
                    {"start": 0.0, "end": 2.5, "text": "Á†ñËØáËÆúËØê ËÆûËÆòËÆ¨ËÆòËΩ¨ËÆ¨. ËÆóËÆù ËÆùËÆóËΩ¨ ËÆõ‰ΩèÈ©ª‰∏ìËÆ¨ËÆ¨ËÆõ Á†ñËØáËÆ±? ËÆªË∞â. ËØ™ËÆõ Á†ñËØáËÆúËØ™ËÆ±? ", "speaker": "SPEAKER_00"},
                    {"start": 3.0, "end": 6.0, "text": "ËØ™ËΩ¨ËÆú‰ΩèËÆªËØáËΩ¨, ËÆóË∞ûËÆ¨ ËØáËÆó ËØ™ËÆúÁà™ËÆóËΩ¨ ËÆóËΩ¨ ËÆõ‰ΩèÈ©ª‰∏ì Á†ñË∞ûËΩ¨ËΩ¨ ËØáËÆ¨ ËÆõËØ™Ë∞ûÁà™ËÆû, ËÆóËÆ¨ËÆ± ËÆùËÆõ Ë∞ûÊãΩ‰∏ìËÆó? ‰ΩèËÆúËÆö ËÆõ‰ΩèËÆ¨È©ªËÆú‰∏ì ËÆõËØ™Ë∞ûÁà™ËÆû? ", "speaker": "SPEAKER_01"},
                    {"start": 7.0, "end": 10.0, "text": "ËÆªË∞â, ËØáËÆó ËÆûÁ†ñËÆúËÆò, ËÆóË∞ûËÆ¨ ËÆóËØ™Áà™ËÆó ËÆóËÆúËΩ¨ËÆú, ËÆóË∞ûËÆ¨ ËÆóËØ™Áà™ËÆó ËÆóËÆúËΩ¨ËÆú. ËØáËÆó, ËÆóË∞ûËÆ¨ ËÆóËÆòËÆ¨ËÆó ËØáËÆ±, ËÆ¨Á†ñ ËØáËÆ¨ ËØ™ËØáËÆó. ËÆóËÆòËØá ËÆªËÆò‰∏ì Ë∞ûËΩ¨ËΩ¨ËÆ¨, ËÆóËÆòËØá ËÆªËÆò‰∏ì Ë∞ûËΩ¨ËΩ¨ËÆ¨ ", "speaker": "SPEAKER_00"},
                    {"start": 10.5, "end": 15.0, "text": "ËØáËÆªËØ™ËÆõ ËÆóËΩ¨ ËÆõ‰ΩèÈ©ª‰∏ì Á†ñËØáËÆ± ËÆ¨ËÆúÁà™ËÆóËÆúËΩ¨ ËØ™ËÆôËÆö‰∏ìË∞â, ËÆúÁ†ñËØ™ËÆûËÆõ ‰∏ìËÆòËÆõ. ËÆóËÆ¨ËÆùËÆõ ËÆªËÆ¨Á•ù, ËÆóË∞ûËÆ¨ ËÆóËÆòËÆ¨ËÆó ËØáËÆ±, ËÆ¨Á†ñ ËØáËÆ¨ ËÆõËØ™ËÆúË∞â ËÆò‰ΩèÈ©ª‰∏ìËÆ¨ËÆ¨ËÆõ", "speaker": "SPEAKER_01"},
                    {"start": 0.0, "end": 2.5, "text": "Á†ñËØáËÆúËØê ËÆûËÆòËÆ¨ËÆòËΩ¨ËÆ¨. ËÆóËÆù ËÆùËÆóËΩ¨ ËÆõ‰ΩèÈ©ª‰∏ìËÆ¨ËÆ¨ËÆõ Á†ñËØáËÆ±? ËÆªË∞â. ËØ™ËÆõ Á†ñËØáËÆúËØ™ËÆ±? ", "speaker": "SPEAKER_00"},
                    {"start": 3.0, "end": 6.0, "text": "ËØ™ËΩ¨ËÆú‰ΩèËÆªËØáËΩ¨, ËÆóË∞ûËÆ¨ ËØáËÆó ËØ™ËÆúÁà™ËÆóËΩ¨ ËÆóËΩ¨ ËÆõ‰ΩèÈ©ª‰∏ì Á†ñË∞ûËΩ¨ËΩ¨ ËØáËÆ¨ ËÆõËØ™Ë∞ûÁà™ËÆû, ËÆóËÆ¨ËÆ± ËÆùËÆõ Ë∞ûÊãΩ‰∏ìËÆó? ‰ΩèËÆúËÆö ËÆõ‰ΩèËÆ¨È©ªËÆú‰∏ì ËÆõËØ™Ë∞ûÁà™ËÆû? ", "speaker": "SPEAKER_01"},
                    {"start": 7.0, "end": 10.0, "text": "ËÆªË∞â, ËØáËÆó ËÆûÁ†ñËÆúËÆò, ËÆóË∞ûËÆ¨ ËÆóËØ™Áà™ËÆó ËÆóËÆúËΩ¨ËÆú, ËÆóË∞ûËÆ¨ ËÆóËØ™Áà™ËÆó ËÆóËÆúËΩ¨ËÆú. ËØáËÆó, ËÆóË∞ûËÆ¨ ËÆóËÆòËÆ¨ËÆó ËØáËÆ±, ËÆ¨Á†ñ ËØáËÆ¨ ËØ™ËØáËÆó. ËÆóËÆòËØá ËÆªËÆò‰∏ì Ë∞ûËΩ¨ËΩ¨ËÆ¨, ËÆóËÆòËØá ËÆªËÆò‰∏ì Ë∞ûËΩ¨ËΩ¨ËÆ¨ ", "speaker": "SPEAKER_00"},
                    {"start": 10.5, "end": 15.0, "text": "ËØáËÆªËØ™ËÆõ ËÆóËΩ¨ ËÆõ‰ΩèÈ©ª‰∏ì Á†ñËØáËÆ± ËÆ¨ËÆúÁà™ËÆóËÆúËΩ¨ ËØ™ËÆôËÆö‰∏ìË∞â, ËÆúÁ†ñËØ™ËÆûËÆõ ‰∏ìËÆòËÆõ. ËÆóËÆ¨ËÆùËÆõ ËÆªËÆ¨Á•ù, ËÆóË∞ûËÆ¨ ËÆóËÆòËÆ¨ËÆó ËØáËÆ±, ËÆ¨Á†ñ ËØáËÆ¨ ËÆõËØ™ËÆúË∞â ËÆò‰ΩèÈ©ª‰∏ìËÆ¨ËÆ¨ËÆõ", "speaker": "SPEAKER_01"},
                    {"start": 0.0, "end": 2.5, "text": "Á†ñËØáËÆúËØê ËÆûËÆòËÆ¨ËÆòËΩ¨ËÆ¨. ËÆóËÆù ËÆùËÆóËΩ¨ ËÆõ‰ΩèÈ©ª‰∏ìËÆ¨ËÆ¨ËÆõ Á†ñËØáËÆ±? ËÆªË∞â. ËØ™ËÆõ Á†ñËØáËÆúËØ™ËÆ±? ", "speaker": "SPEAKER_00"},
                    {"start": 3.0, "end": 6.0, "text": "ËØ™ËΩ¨ËÆú‰ΩèËÆªËØáËΩ¨, ËÆóË∞ûËÆ¨ ËØáËÆó ËØ™ËÆúÁà™ËÆóËΩ¨ ËÆóËΩ¨ ËÆõ‰ΩèÈ©ª‰∏ì Á†ñË∞ûËΩ¨ËΩ¨ ËØáËÆ¨ ËÆõËØ™Ë∞ûÁà™ËÆû, ËÆóËÆ¨ËÆ± ËÆùËÆõ Ë∞ûÊãΩ‰∏ìËÆó? ‰ΩèËÆúËÆö ËÆõ‰ΩèËÆ¨È©ªËÆú‰∏ì ËÆõËØ™Ë∞ûÁà™ËÆû? ", "speaker": "SPEAKER_01"},
                    {"start": 7.0, "end": 10.0, "text": "ËÆªË∞â, ËØáËÆó ËÆûÁ†ñËÆúËÆò, ËÆóË∞ûËÆ¨ ËÆóËØ™Áà™ËÆó ËÆóËÆúËΩ¨ËÆú, ËÆóË∞ûËÆ¨ ËÆóËØ™Áà™ËÆó ËÆóËÆúËΩ¨ËÆú. ËØáËÆó, ËÆóË∞ûËÆ¨ ËÆóËÆòËÆ¨ËÆó ËØáËÆ±, ËÆ¨Á†ñ ËØáËÆ¨ ËØ™ËØáËÆó. ËÆóËÆòËØá ËÆªËÆò‰∏ì Ë∞ûËΩ¨ËΩ¨ËÆ¨, ËÆóËÆòËØá ËÆªËÆò‰∏ì Ë∞ûËΩ¨ËΩ¨ËÆ¨ ", "speaker": "SPEAKER_00"},
                    {"start": 10.5, "end": 15.0, "text": "ËØáËÆªËØ™ËÆõ ËÆóËΩ¨ ËÆõ‰ΩèÈ©ª‰∏ì Á†ñËØáËÆ± ËÆ¨ËÆúÁà™ËÆóËÆúËΩ¨ ËØ™ËÆôËÆö‰∏ìË∞â, ËÆúÁ†ñËØ™ËÆûËÆõ ‰∏ìËÆòËÆõ. ËÆóËÆ¨ËÆùËÆõ ËÆªËÆ¨Á•ù, ËÆóË∞ûËÆ¨ ËÆóËÆòËÆ¨ËÆó ËØáËÆ±, ËÆ¨Á†ñ ËØáËÆ¨ ËÆõËØ™ËÆúË∞â ËÆò‰ΩèÈ©ª‰∏ìËÆ¨ËÆ¨ËÆõ", "speaker": "SPEAKER_01"},
                    {"start": 0.0, "end": 2.5, "text": "Á†ñËØáËÆúËØê ËÆûËÆòËÆ¨ËÆòËΩ¨ËÆ¨. ËÆóËÆù ËÆùËÆóËΩ¨ ËÆõ‰ΩèÈ©ª‰∏ìËÆ¨ËÆ¨ËÆõ Á†ñËØáËÆ±? ËÆªË∞â. ËØ™ËÆõ Á†ñËØáËÆúËØ™ËÆ±? ", "speaker": "SPEAKER_00"},
                    {"start": 3.0, "end": 6.0, "text": "ËØ™ËΩ¨ËÆú‰ΩèËÆªËØáËΩ¨, ËÆóË∞ûËÆ¨ ËØáËÆó ËØ™ËÆúÁà™ËÆóËΩ¨ ËÆóËΩ¨ ËÆõ‰ΩèÈ©ª‰∏ì Á†ñË∞ûËΩ¨ËΩ¨ ËØáËÆ¨ ËÆõËØ™Ë∞ûÁà™ËÆû, ËÆóËÆ¨ËÆ± ËÆùËÆõ Ë∞ûÊãΩ‰∏ìËÆó? ‰ΩèËÆúËÆö ËÆõ‰ΩèËÆ¨È©ªËÆú‰∏ì ËÆõËØ™Ë∞ûÁà™ËÆû? ", "speaker": "SPEAKER_01"},
                    {"start": 7.0, "end": 10.0, "text": "ËÆªË∞â, ËØáËÆó ËÆûÁ†ñËÆúËÆò, ËÆóË∞ûËÆ¨ ËÆóËØ™Áà™ËÆó ËÆóËÆúËΩ¨ËÆú, ËÆóË∞ûËÆ¨ ËÆóËØ™Áà™ËÆó ËÆóËÆúËΩ¨ËÆú. ËØáËÆó, ËÆóË∞ûËÆ¨ ËÆóËÆòËÆ¨ËÆó ËØáËÆ±, ËÆ¨Á†ñ ËØáËÆ¨ ËØ™ËØáËÆó. ËÆóËÆòËØá ËÆªËÆò‰∏ì Ë∞ûËΩ¨ËΩ¨ËÆ¨, ËÆóËÆòËØá ËÆªËÆò‰∏ì Ë∞ûËΩ¨ËΩ¨ËÆ¨ ", "speaker": "SPEAKER_00"},
                    {"start": 10.5, "end": 15.0, "text": "ËØáËÆªËØ™ËÆõ ËÆóËΩ¨ ËÆõ‰ΩèÈ©ª‰∏ì Á†ñËØáËÆ± ËÆ¨ËÆúÁà™ËÆóËÆúËΩ¨ ËØ™ËÆôËÆö‰∏ìË∞â, ËÆúÁ†ñËØ™ËÆûËÆõ ‰∏ìËÆòËÆõ. ËÆóËÆ¨ËÆùËÆõ ËÆªËÆ¨Á•ù, ËÆóË∞ûËÆ¨ ËÆóËÆòËÆ¨ËÆó ËØáËÆ±, ËÆ¨Á†ñ ËØáËÆ¨ ËÆõËØ™ËÆúË∞â ËÆò‰ΩèÈ©ª‰∏ìËÆ¨ËÆ¨ËÆõ", "speaker": "SPEAKER_01"},
                    {"start": 0.0, "end": 2.5, "text": "Á†ñËØáËÆúËØê ËÆûËÆòËÆ¨ËÆòËΩ¨ËÆ¨. ËÆóËÆù ËÆùËÆóËΩ¨ ËÆõ‰ΩèÈ©ª‰∏ìËÆ¨ËÆ¨ËÆõ Á†ñËØáËÆ±? ËÆªË∞â. ËØ™ËÆõ Á†ñËØáËÆúËØ™ËÆ±? ", "speaker": "SPEAKER_00"},
                    {"start": 3.0, "end": 6.0, "text": "ËØ™ËΩ¨ËÆú‰ΩèËÆªËØáËΩ¨, ËÆóË∞ûËÆ¨ ËØáËÆó ËØ™ËÆúÁà™ËÆóËΩ¨ ËÆóËΩ¨ ËÆõ‰ΩèÈ©ª‰∏ì Á†ñË∞ûËΩ¨ËΩ¨ ËØáËÆ¨ ËÆõËØ™Ë∞ûÁà™ËÆû, ËÆóËÆ¨ËÆ± ËÆùËÆõ Ë∞ûÊãΩ‰∏ìËÆó? ‰ΩèËÆúËÆö ËÆõ‰ΩèËÆ¨È©ªËÆú‰∏ì ËÆõËØ™Ë∞ûÁà™ËÆû? ", "speaker": "SPEAKER_01"},
                    {"start": 7.0, "end": 10.0, "text": "ËÆªË∞â, ËØáËÆó ËÆûÁ†ñËÆúËÆò, ËÆóË∞ûËÆ¨ ËÆóËØ™Áà™ËÆó ËÆóËÆúËΩ¨ËÆú, ËÆóË∞ûËÆ¨ ËÆóËØ™Áà™ËÆó ËÆóËÆúËΩ¨ËÆú. ËØáËÆó, ËÆóË∞ûËÆ¨ ËÆóËÆòËÆ¨ËÆó ËØáËÆ±, ËÆ¨Á†ñ ËØáËÆ¨ ËØ™ËØáËÆó. ËÆóËÆòËØá ËÆªËÆò‰∏ì Ë∞ûËΩ¨ËΩ¨ËÆ¨, ËÆóËÆòËØá ËÆªËÆò‰∏ì Ë∞ûËΩ¨ËΩ¨ËÆ¨ ", "speaker": "SPEAKER_00"},
                    {"start": 10.5, "end": 15.0, "text": "ËØáËÆªËØ™ËÆõ ËÆóËΩ¨ ËÆõ‰ΩèÈ©ª‰∏ì Á†ñËØáËÆ± ËÆ¨ËÆúÁà™ËÆóËÆúËΩ¨ ËØ™ËÆôËÆö‰∏ìË∞â, ËÆúÁ†ñËØ™ËÆûËÆõ ‰∏ìËÆòËÆõ. ËÆóËÆ¨ËÆùËÆõ ËÆªËÆ¨Á•ù, ËÆóË∞ûËÆ¨ ËÆóËÆòËÆ¨ËÆó ËØáËÆ±, ËÆ¨Á†ñ ËØáËÆ¨ ËÆõËØ™ËÆúË∞â ËÆò‰ΩèÈ©ª‰∏ìËÆ¨ËÆ¨ËÆõ", "speaker": "SPEAKER_01"}
                ]
            }
        })
    # Check the global cache we created earlier
    if job_id in job_results_cache:
        print(f"È¶ÉÊî∑ Client checked status for {job_id} -> Found completed result!")
        return jsonify(job_results_cache[job_id])

    # If not in cache, it's still processing (or lost, but let's assume processing)
    return jsonify({"status": "processing"}), 202

# --- GPU FEEDBACK API ---
# --- 1. Add Global Cache at the top ---
job_results_cache = {}

# --- 2. Update GPU Callback to SAVE the data ---
# Inside siteapp.py -> gpu_callback
@app.route('/api/gpu_callback', methods=['POST'])
def gpu_callback():
    data = request.json
    job_id = data.get('jobId')

    # Store in cache for persistence
    job_results_cache[job_id] = data

    # EMIT: 'job_status_update' must match the listener in app_logic.js
    socketio.emit('job_status_update', data, room=job_id)
    print(f"DEBUG: Emitted result to room {job_id}")

    return jsonify({"status": "ok"}), 200


# --- 3. Update Join Logic to CHECK the Cache ---
@socketio.on('join')
def on_join(data):
    room = data.get('room')
    if room:
        join_room(room)
        print(f"È¶ÉÊî≤ Client joined room: {room}")

        # CHECK MAILBOX: Is the result already waiting?
        if room in job_results_cache:
            print(f"È¶ÉÊëù Found cached result for {room}, sending now!")
            # Send it to this specific user who just reconnected
            socketio.emit('job_status_update', job_results_cache[room], room=request.sid)

# --- SIMULATION BACKGROUND TASK ---
# This simulates the GPU finishing and sending data back after 4 seconds
# Updated simulation thread logic
def simulate_completion(jid, run_diarization):
    import time
    time.sleep(1)
    segments = []

    for i in range(1, 11):
        if not run_diarization:
            segments.append({
                "start": float(i * 10),
                "end": float(i * 10 + 5),
                "text": f"Line {i}: Plain text testing for the one-pager layout."
            })
        else:
            # IMPORTANT: The key MUST be "speaker" (lowercase)
            segments.append({
                "start": float(i * 10),
                "end": float(i * 10 + 4),
                "text": f"Speaker 1 Message {i}: Scroll testing.",
                "speaker": "SPEAKER_00"
            })
            segments.append({
                "start": float(i * 10 + 5),
                "end": float(i * 10 + 9),
                "text": f"Speaker 2 Message {i}: More text to fill space.",
                "speaker": "SPEAKER_01"
            })

    mock_data = {
        "jobId": jid,
        "status": "completed",
        "result": {"segments": segments}
    }

    # Use the global variables explicitly to avoid thread scope issues
    global job_results_cache
    job_results_cache[jid] = mock_data

    # This sends the message that clears the "Processing" button
    socketio.emit('job_status_update', mock_data, room=jid)
    print(f"üîÆ SIMULATION COMPLETE: Room {jid} | Diarization: {run_diarization}")
@app.route('/api/sign-s3', methods=['POST'])
def sign_s3():
    import boto3
    import os
    import time
    from threading import Thread

    if SIMULATION_MODE:
        job_id = f"job_sim_{int(time.time())}"

        # --- FIX: Extract the diarization flag from the request ---
        data = request.json or {}
        is_diarization_requested = data.get('diarization', False)

        # --- FIX: Pass BOTH arguments to the thread ---
        Thread(target=simulate_completion, args=(job_id, is_diarization_requested)).start()

        return jsonify({
            'data': {
                'url': 'http://localhost:8000/api/mock-upload',
                's3Key': 'simulation_key',
                'jobId': job_id
            }
        })

    else:
        # --- LIVE AWS LOGIC ---
        data = request.json
        filename = data.get('filename')
        file_type = data.get('filetype')

        key_id = os.environ.get("AWS_ACCESS_KEY_ID")
        secret = os.environ.get("AWS_SECRET_ACCESS_KEY")
        region = os.environ.get("AWS_REGION", "eu-north-1")
        bucket = os.environ.get("S3_BUCKET") or "quickscribe-v2-12345"

        if not key_id or not secret:
            return jsonify({"status": "error", "message": "AWS Credentials missing on server"}), 500

        s3_client = boto3.client(
            's3',
            aws_access_key_id=key_id,
            aws_secret_access_key=secret,
            region_name=region
        )

        base_name, extension = os.path.splitext(filename)
        job_id = f"job_{int(time.time())}_{base_name}"
        s3_key = f"input/{job_id}{extension}"

        presigned_url = s3_client.generate_presigned_url(
            'put_object',
            Params={
                'Bucket': bucket,
                'Key': s3_key,
                'ContentType': file_type
            },
            ExpiresIn=3600
        )

        return jsonify({
            'data': {
                'url': presigned_url,
                's3Key': s3_key,
                'jobId': job_id
            }
        })


@app.route('/api/trigger_processing', methods=['POST'])
def trigger_processing():
    try:
        if SIMULATION_MODE:
            print("üîÆ SIMULATION: Skipping RunPod Trigger")
            return jsonify({"status": "started", "runpod_id": "sim_id_123"})

        data = request.json
        print(f"üì© Received Trigger Request: {data}")

        # --- 1. GET CREDENTIALS & CHECK THEM ---
        endpoint_id = os.environ.get('RUNPOD_ENDPOINT_ID')
        api_key = os.environ.get('RUNPOD_API_KEY')

        print(f"üîë checking keys... Endpoint ID exists? {bool(endpoint_id)} | API Key exists? {bool(api_key)}")

        if not endpoint_id or not api_key:
            return jsonify({"status": "error", "message": "RunPod Env Vars missing on server"}), 500

        # --- PREPARE DATA ---
        s3_key = data.get('s3Key')
        job_id = data.get('jobId')
        task = data.get('task', 'transcribe')
        language = data.get('language', 'he')
        diarization = data.get('diarization', False)

        try:
            speaker_count = int(data.get('speakerCount', 2))
        except:
            speaker_count = 2

        # --- BUILD PAYLOAD ---
        payload = {
            "input": {
                "s3Key": s3_key,
                "jobId": job_id,
                "task": task,
                "language": language,
                "num_speakers": speaker_count,
                "diarization": diarization
            }
        }

        # Clean the ID to prevent URL errors
        clean_id = str(endpoint_id).strip()
        endpoint_url = f"https://api.runpod.ai/v2/{clean_id}/run"

        headers = {
            "Authorization": f"Bearer {api_key.strip()}",
            "Content-Type": "application/json"
        }

        print(f"üöÄ Connecting to RunPod URL: {endpoint_url}")

        # Timeout added to prevent the 500 error from a hanging connection
        response = requests.post(endpoint_url, json=payload, headers=headers, timeout=15)

        if response.status_code != 200:
            print(f"‚ùå RunPod API Error ({response.status_code}): {response.text}")
            return jsonify({"status": "error", "message": f"RunPod API Rejected Request: {response.status_code}"}), 500

        return jsonify({"status": "started", "runpod_id": response.json().get('id')})

    except Exception as e:
        print(f"‚ùå trigger_processing CRASHED: {str(e)}")
        import traceback
        traceback.print_exc()  # This will show the exact line of the crash in Koyeb logs
        return jsonify({"status": "error", "message": str(e)}), 500

# --- WEBSOCKET EVENT HANDLERS ---
@socketio.on('connect')
def handle_connect():
    job_id = request.args.get('jobId')
    if job_id:
        join_room(job_id)
        print(f"CLIENT CONNECTED: Joined room {job_id}")

@socketio.on('disconnect')
def handle_disconnect():
    print("CLIENT DISCONNECTED")

# --- HEALTH CHECK ROUTE ---
@app.route('/health')
def health_check():
    return "OK", 200

if __name__ == '__main__':
    port = int(os.environ.get("PORT", 8000))
    socketio.run(app, host='0.0.0.0', port=port)